{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2273874681d84a65a58c946814c9e68e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e43f30e73d10437eae0cf02b297f5803",
              "IPY_MODEL_1cf79185790d41e280a1471f1bd53e5c",
              "IPY_MODEL_f7c98095dcce43f8920726b55c8b4b6b"
            ],
            "layout": "IPY_MODEL_19c5afc80e3a445992de4a7a849a48b2"
          }
        },
        "e43f30e73d10437eae0cf02b297f5803": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3560fc744eed4b90a39a2774db515ccb",
            "placeholder": "​",
            "style": "IPY_MODEL_0c9e28fd5de249f897486c101b745a62",
            "value": "Fetching 5 files: 100%"
          }
        },
        "1cf79185790d41e280a1471f1bd53e5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ab47fbc58a04d3d828ef630099235bf",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_07f83f36688b4176bdadebb53da13740",
            "value": 5
          }
        },
        "f7c98095dcce43f8920726b55c8b4b6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd8ef3bbb30c4b18bf2e270df2c15e1d",
            "placeholder": "​",
            "style": "IPY_MODEL_25a291f7186a4752ae374750b1293f71",
            "value": " 5/5 [00:00&lt;00:00, 216.69it/s]"
          }
        },
        "19c5afc80e3a445992de4a7a849a48b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3560fc744eed4b90a39a2774db515ccb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c9e28fd5de249f897486c101b745a62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ab47fbc58a04d3d828ef630099235bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07f83f36688b4176bdadebb53da13740": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd8ef3bbb30c4b18bf2e270df2c15e1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25a291f7186a4752ae374750b1293f71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Chunking Techniques in Language Models**\n",
        "\n",
        "\n",
        "Large Language Models (LLMs) can suffer from a problem known as hallucination, where the model generates incorrect or nonsensical information. To mitigate this issue, it is important to stay within the LLM's context window. One approach is to break texts into smaller parts, a process known as chunking. This technique improves the accuracy and efficiency of the retrieval process and enhances the overall performance of Retrieval-Augmented Generation (RAG) models.\n",
        "\n",
        "**Types of Chunking Techniques**\n",
        "1. Recursive Chunking\n",
        "\n",
        "  Recursive chunking involves breaking down tasks or information into smaller parts in a hierarchical structure. Each chunk can be further divided into sub-chunks recursively. This top-down approach allows larger chunks to be split into smaller, manageable units.\n",
        "\n",
        "2. Semantic Chunking\n",
        "\n",
        "  Semantic chunking focuses on breaking information into meaningful or contextually related chunks. This method emphasizes the structure and understanding of the data. It is typically slower than recursive chunking due to its focus on meaning and comprehension.\n"
      ],
      "metadata": {
        "id": "tbcII_Pdzp8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install the required dependencies"
      ],
      "metadata": {
        "id": "r0MGI83JzXQq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ThTNTUugzTFZ"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain_experimental langchain_openai langchain_community langchain ragas chromadb langchain-groq fastembed pypdf openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Imports"
      ],
      "metadata": {
        "id": "N8k6PsCY7gjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
        "from google.colab import userdata\n",
        "from groq import Groq\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.vectorstores import Chroma\n",
        "groq_api_key = userdata.get(\"Groq\")\n",
        "embed_model = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "2273874681d84a65a58c946814c9e68e",
            "e43f30e73d10437eae0cf02b297f5803",
            "1cf79185790d41e280a1471f1bd53e5c",
            "f7c98095dcce43f8920726b55c8b4b6b",
            "19c5afc80e3a445992de4a7a849a48b2",
            "3560fc744eed4b90a39a2774db515ccb",
            "0c9e28fd5de249f897486c101b745a62",
            "2ab47fbc58a04d3d828ef630099235bf",
            "07f83f36688b4176bdadebb53da13740",
            "cd8ef3bbb30c4b18bf2e270df2c15e1d",
            "25a291f7186a4752ae374750b1293f71"
          ]
        },
        "id": "OeratzAL7iRF",
        "outputId": "c378c3e8-01f8-4168-d2e9-461c751f35d8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2273874681d84a65a58c946814c9e68e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Fetch Sample Docs"
      ],
      "metadata": {
        "id": "bEOWPlOxN9Cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget \"https://arxiv.org/pdf/1810.04805.pdf\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE8ayVn1OktF",
        "outputId": "04107ac5-71f4-434f-859a-579ca3a890a5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-23 05:55:26--  https://arxiv.org/pdf/1810.04805.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.195.42, 151.101.3.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/1810.04805 [following]\n",
            "--2024-10-23 05:55:26--  http://arxiv.org/pdf/1810.04805\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 775166 (757K) [application/pdf]\n",
            "Saving to: ‘1810.04805.pdf.1’\n",
            "\n",
            "\r1810.04805.pdf.1      0%[                    ]       0  --.-KB/s               \r1810.04805.pdf.1    100%[===================>] 757.00K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-10-23 05:55:26 (36.1 MB/s) - ‘1810.04805.pdf.1’ saved [775166/775166]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Process the PDF Content"
      ],
      "metadata": {
        "id": "HJr9q5HGOouw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"1810.04805.pdf\")\n",
        "documents = loader.load()\n",
        "\n",
        "print(\"Number of pages:\", len(documents))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MN6sfiBOo2u",
        "outputId": "62bd38c2-fa84-405d-9214-814c02889d60"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of pages: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Recursive Chunking(Notice the overlap in clunk 1 and 2)"
      ],
      "metadata": {
        "id": "ioUNBePBQpq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size for demo\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "recursive_texts = text_splitter.split_documents(documents)\n",
        "print(\"Chunk 1: \", recursive_texts[0].page_content)\n",
        "print(\"Chunk 2: \", recursive_texts[1].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8MrGVkVQpxr",
        "outputId": "c2507e18-c94b-4d7a-a673-2a8d583d2b2e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1:  BERT: Pre-training of Deep Bidirectional Transformers for\n",
            "Language Understanding\n",
            "Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\n",
            "Google AI Language\n",
            "{jacobdevlin,mingweichang,kentonl,kristout }@google.com\n",
            "Abstract\n",
            "We introduce a new language representa-\n",
            "tion model called BERT , which stands for\n",
            "Bidirectional Encoder Representations from\n",
            "Transformers. Unlike recent language repre-\n",
            "sentation models (Peters et al., 2018a; Rad-\n",
            "ford et al., 2018), BERT is designed to pre-\n",
            "train deep bidirectional representations from\n",
            "unlabeled text by jointly conditioning on both\n",
            "left and right context in all layers. As a re-\n",
            "sult, the pre-trained BERT model can be ﬁne-\n",
            "tuned with just one additional output layer\n",
            "to create state-of-the-art models for a wide\n",
            "range of tasks, such as question answering and\n",
            "language inference, without substantial task-\n",
            "speciﬁc architecture modiﬁcations.\n",
            "BERT is conceptually simple and empirically\n",
            "powerful. It obtains new state-of-the-art re-\n",
            "Chunk 2:  BERT is conceptually simple and empirically\n",
            "powerful. It obtains new state-of-the-art re-\n",
            "sults on eleven natural language processing\n",
            "tasks, including pushing the GLUE score to\n",
            "80.5% (7.7% point absolute improvement),\n",
            "MultiNLI accuracy to 86.7% (4.6% absolute\n",
            "improvement), SQuAD v1.1 question answer-\n",
            "ing Test F1 to 93.2 (1.5 point absolute im-\n",
            "provement) and SQuAD v2.0 Test F1 to 83.1\n",
            "(5.1 point absolute improvement).\n",
            "1 Introduction\n",
            "Language model pre-training has been shown to\n",
            "be effective for improving many natural language\n",
            "processing tasks (Dai and Le, 2015; Peters et al.,\n",
            "2018a; Radford et al., 2018; Howard and Ruder,\n",
            "2018). These include sentence-level tasks such as\n",
            "natural language inference (Bowman et al., 2015;\n",
            "Williams et al., 2018) and paraphrasing (Dolan\n",
            "and Brockett, 2005), which aim to predict the re-\n",
            "lationships between sentences by analyzing them\n",
            "holistically, as well as token-level tasks such as\n",
            "named entity recognition and question answering,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Semantic Chunking(Percentile)"
      ],
      "metadata": {
        "id": "pH0ZIyqUg_NX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = SemanticChunker(embed_model, breakpoint_threshold_type=\"percentile\")\n",
        "\n",
        "semantic_texts = text_splitter.split_documents(documents)\n",
        "print(semantic_texts[0].page_content)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiEECPPCg_XV",
        "outputId": "466a0528-cc39-4caa-e06a-97c15ecf21a4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT: Pre-training of Deep Bidirectional Transformers for\n",
            "Language Understanding\n",
            "Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\n",
            "Google AI Language\n",
            "{jacobdevlin,mingweichang,kentonl,kristout }@google.com\n",
            "Abstract\n",
            "We introduce a new language representa-\n",
            "tion model called BERT , which stands for\n",
            "Bidirectional Encoder Representations from\n",
            "Transformers. Unlike recent language repre-\n",
            "sentation models (Peters et al., 2018a; Rad-\n",
            "ford et al., 2018), BERT is designed to pre-\n",
            "train deep bidirectional representations from\n",
            "unlabeled text by jointly conditioning on both\n",
            "left and right context in all layers. As a re-\n",
            "sult, the pre-trained BERT model can be ﬁne-\n",
            "tuned with just one additional output layer\n",
            "to create state-of-the-art models for a wide\n",
            "range of tasks, such as question answering and\n",
            "language inference, without substantial task-\n",
            "speciﬁc architecture modiﬁcations. BERT is conceptually simple and empirically\n",
            "powerful. It obtains new state-of-the-art re-\n",
            "sults on eleven natural language processing\n",
            "tasks, including pushing the GLUE score to\n",
            "80.5% (7.7% point absolute improvement),\n",
            "MultiNLI accuracy to 86.7% (4.6% absolute\n",
            "improvement), SQuAD v1.1 question answer-\n",
            "ing Test F1 to 93.2 (1.5 point absolute im-\n",
            "provement) and SQuAD v2.0 Test F1 to 83.1\n",
            "(5.1 point absolute improvement). 1 Introduction\n",
            "Language model pre-training has been shown to\n",
            "be effective for improving many natural language\n",
            "processing tasks (Dai and Le, 2015; Peters et al.,\n",
            "2018a; Radford et al., 2018; Howard and Ruder,\n",
            "2018). These include sentence-level tasks such as\n",
            "natural language inference (Bowman et al., 2015;\n",
            "Williams et al., 2018) and paraphrasing (Dolan\n",
            "and Brockett, 2005), which aim to predict the re-\n",
            "lationships between sentences by analyzing them\n",
            "holistically, as well as token-level tasks such as\n",
            "named entity recognition and question answering,\n",
            "where models are required to produce ﬁne-grained\n",
            "output at the token level (Tjong Kim Sang and\n",
            "De Meulder, 2003; Rajpurkar et al., 2016).There are two existing strategies for apply-\n",
            "ing pre-trained language representations to down-\n",
            "stream tasks: feature-based andﬁne-tuning . The\n",
            "feature-based approach, such as ELMo (Peters\n",
            "et al., 2018a), uses task-speciﬁc architectures that\n",
            "include the pre-trained representations as addi-\n",
            "tional features. The ﬁne-tuning approach, such as\n",
            "the Generative Pre-trained Transformer (OpenAI\n",
            "GPT) (Radford et al., 2018), introduces minimal\n",
            "task-speciﬁc parameters, and is trained on the\n",
            "downstream tasks by simply ﬁne-tuning allpre-\n",
            "trained parameters. The two approaches share the\n",
            "same objective function during pre-training, where\n",
            "they use unidirectional language models to learn\n",
            "general language representations. We argue that current techniques restrict the\n",
            "power of the pre-trained representations, espe-\n",
            "cially for the ﬁne-tuning approaches. The ma-\n",
            "jor limitation is that standard language models are\n",
            "unidirectional, and this limits the choice of archi-\n",
            "tectures that can be used during pre-training. For\n",
            "example, in OpenAI GPT, the authors use a left-to-\n",
            "right architecture, where every token can only at-\n",
            "tend to previous tokens in the self-attention layers\n",
            "of the Transformer (Vaswani et al., 2017).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Instantiate the Vectorstore"
      ],
      "metadata": {
        "id": "wlBTA4jeBHUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recursive_chunk_vectorstore = Chroma.from_documents(recursive_texts, embedding=embed_model)\n",
        "semantic_chunk_vectorstore = Chroma.from_documents(semantic_texts, embedding=embed_model)\n"
      ],
      "metadata": {
        "id": "-xXbApx2BHfN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Instantiate Retrieval(Recursive Chunking)\n"
      ],
      "metadata": {
        "id": "VgR2zC7EBH6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recursive_chunk_retriever = recursive_chunk_vectorstore.as_retriever(search_kwargs={\"k\" : 1})\n",
        "recursive_chunk_retriever.invoke(\"What is BERT is designed for?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpkyaI7d7yJN",
        "outputId": "e5e3d3af-69b1-4a1d-f412-b365f393fadf"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'page': 6, 'source': '1810.04805.pdf'}, page_content='BERT LARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2\\nTable 2: SQuAD 1.1 results. The BERT ensemble\\nis 7x systems which use different pre-training check-\\npoints and ﬁne-tuning seeds.\\nSystem Dev Test\\nEM F1 EM F1\\nTop Leaderboard Systems (Dec 10th, 2018)\\nHuman 86.3 89.0 86.9 89.5\\n#1 Single - MIR-MRC (F-Net) - - 74.8 78.0\\n#2 Single - nlnet - - 74.2 77.1\\nPublished\\nunet (Ensemble) - - 71.4 74.9\\nSLQA+ (Single) - 71.4 74.4\\nOurs\\nBERT LARGE (Single) 78.7 81.9 80.0 83.1')]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Instantiate Retrieval(Semantic Chunking)"
      ],
      "metadata": {
        "id": "j6_tA8CoBFvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_chunk_retriever = semantic_chunk_vectorstore.as_retriever(search_kwargs={\"k\" : 1})\n",
        "semantic_chunk_retriever.invoke(\"What is BERT is designed for?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLk3GYAWCtJX",
        "outputId": "d87fe381-f17b-494c-858e-f07a33f88152"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'page': 6, 'source': '1810.04805.pdf'}, page_content='BERT LARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2\\nTable 2: SQuAD 1.1 results. The BERT ensemble\\nis 7x systems which use different pre-training check-\\npoints and ﬁne-tuning seeds.\\nSystem Dev Test\\nEM F1 EM F1\\nTop Leaderboard Systems (Dec 10th, 2018)\\nHuman 86.3 89.0 86.9 89.5\\n#1 Single - MIR-MRC (F-Net) - - 74.8 78.0\\n#2 Single - nlnet - - 74.2 77.1\\nPublished\\nunet (Ensemble) - - 71.4 74.9\\nSLQA+ (Single) - 71.4 74.4\\nOurs\\nBERT LARGE (Single) 78.7 81.9 80.0 83.1')]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Implementation into RAG(Semantic Chunking)"
      ],
      "metadata": {
        "id": "bGlEgHcgKS6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "rag_template = \"\"\"\\\n",
        "Use the following context to answer the user's query. If you cannot answer, please respond with 'I don't know'.\n",
        "\n",
        "User's Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
        "chat_model = ChatGroq(temperature=0,\n",
        "                      model_name=\"mixtral-8x7b-32768\",\n",
        "                      api_key=userdata.get(\"Groq\"),)\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "semantic_rag_chain = (\n",
        "    {\"context\" : semantic_chunk_retriever, \"question\" : RunnablePassthrough()}\n",
        "    | rag_prompt\n",
        "    | chat_model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "semantic_rag_chain.invoke(\"What is BERT is designed for?\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "tOynBRTrJCfz",
        "outputId": "60854159-0363-4781-f2e0-bae0bd60541d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BERT (Bidirectional Encoder Representations from Transformers) is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. This means that BERT can understand the context of a word by looking at the words that come before and after it. After pre-training, the BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Implementation into RAG(Recursive Chunking)"
      ],
      "metadata": {
        "id": "DwrhXD3bKYvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "recursive_rag_chain = (\n",
        "    {\"context\" : recursive_chunk_retriever, \"question\" : RunnablePassthrough()}\n",
        "    | rag_prompt\n",
        "    | chat_model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "recursive_rag_chain.invoke(\"What is BERT is designed for?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "_MNtVvIVJV6f",
        "outputId": "276a858f-853b-4b4a-e4e3-db51d7a43569"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BERT (Bidirectional Encoder Representations from Transformers) is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. The pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. It consists of two steps: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "Although the result from both chunking techniques are almost the same, recursive chunking does provide more factual representation of the answer such as elaborating on the two stages of the BERT model while the Semantic chunking technique approach gave a more concise, high-level explanation of BERT’s design.The differences between the two outputs indicate that recursive chunking is better at getting extensive information, and semantic chunking excels at providing more focused, high-level responses. Depending on the purpose, one strategy may be more appropriate than another."
      ],
      "metadata": {
        "id": "ottFXFNaKKfu"
      }
    }
  ]
}